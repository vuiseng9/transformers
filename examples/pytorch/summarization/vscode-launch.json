{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "ft Pegasus for xsum", 
            // this model has been pretrained and released by official HF
            // https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad
            // f1 = 93.1584
            // exact_match = 86.91
            // eval_samples = 10784
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "2",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "summarization",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/summarization/",
            "program": "run_summarization.py",
            "args": [
                // "--model_name_or_path", "facebook/bart-large-cnn",
                // "--model_name_or_path", "MohamedZaitoon/T5-CNN",
                // https://huggingface.co/models?search=google/pegasus
                // "--model_name_or_path", "google/pegasus-cnn_dailymail",
                // "--dataset_name", "cnn_dailymail",
                // "--dataset_config", "3.
                "--model_name_or_path", "google/pegasus-large",
                "--dataset_name", "xsum",
                "--max_source_length", "512",
                "--max_target_length", "64",
                "--run_name", "ft-pegasus-xsum",
                // "--source_prefix", "summarize: ",
                "--adafactor",
                "--learning_rate", "1e-4",
                "--label_smoothing_factor", "0.1",
                "--num_train_epochs", "5",
                "--do_train",
                "--per_device_train_batch_size", "8",
                "--do_eval",
                "--per_device_eval_batch_size", "8",
                "--predict_with_generate",
                "--num_beams", "8",
                "--evaluation_strategy", "steps",
                "--eval_steps", "5",
                // "--save_steps", "5", // only save once
                "--overwrite_output_dir",
                "--output_dir", "/tmp/vscode-runs/pegasus"
            ]
        },
        {
            "name": "Eval Pegasus (arxiv)", 
            // this model has been pretrained and released by official HF
            // https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad
            // f1 = 93.1584
            // exact_match = 86.91
            // eval_samples = 10784
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "2",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "summarization",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/summarization/",
            "program": "run_summarization.py",
            "args": [
                "--model_name_or_path", "google/pegasus-arxiv",
                "--dataset_name", "arxiv_dataset",
                "--dataset_dir", "/data/dataset/arxiv",
                "--max_source_length", "1024",
                "--max_target_length", "256",
                "--run_name", "pegasus-arxiv",
                "--do_eval",
                "--predict_with_generate",
                "--num_beams", "8",
                "--per_device_eval_batch_size", "8",
                "--overwrite_output_dir",
                "--output_dir", "/tmp/vscode-runs/pegasus"
            ]
        },
        {
            "name": "Eval Pegasus (billsum)", 
            // this model has been pretrained and released by official HF
            // https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad
            // f1 = 93.1584
            // exact_match = 86.91
            // eval_samples = 10784
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "2",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "summarization",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/summarization/",
            "program": "run_summarization.py",
            "args": [
                "--model_name_or_path", "google/pegasus-billsum",
                "--dataset_name", "billsum",
                "--max_source_length", "1024",
                "--max_target_length", "256",
                "--run_name", "pegasus-billsum",
                "--do_eval",
                "--predict_with_generate",
                "--num_beams", "8",
                "--per_device_eval_batch_size", "8",
                "--overwrite_output_dir",
                "--output_dir", "/tmp/vscode-runs/pegasus"
            ]
        },
        {
            "name": "Eval Pegasus (wikihow)", 
            // this model has been pretrained and released by official HF
            // https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad
            // f1 = 93.1584
            // exact_match = 86.91
            // eval_samples = 10784
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "2",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "summarization",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/summarization/",
            "program": "run_summarization.py",
            "args": [
                "--model_name_or_path", "google/pegasus-wikihow",
                "--dataset_name", "wikihow",
                "--dataset_config", "all",
                "--dataset_dir", "/data/dataset/wikihow/",
                "--max_source_length", "512",
                "--max_target_length", "256",
                "--run_name", "pegasus-wikihow",
                "--do_eval",
                "--predict_with_generate",
                "--num_beams", "8",
                "--per_device_eval_batch_size", "8",
                "--overwrite_output_dir",
                "--output_dir", "/tmp/vscode-runs/pegasus"
            ]
        },
        {
            "name": "Eval Pegasus", 
            // this model has been pretrained and released by official HF
            // https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad
            // f1 = 93.1584
            // exact_match = 86.91
            // eval_samples = 10784
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "2",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "summarization",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/summarization/",
            "program": "run_summarization.py",
            "args": [
                // "--model_name_or_path", "facebook/bart-large-cnn",
                // "--model_name_or_path", "MohamedZaitoon/T5-CNN",
                // https://huggingface.co/models?search=google/pegasus
                // "--model_name_or_path", "google/pegasus-cnn_dailymail",
                // "--dataset_name", "cnn_dailymail",
                // "--dataset_config", "3.0.0",
                "--model_name_or_path", "google/pegasus-xsum",
                "--dataset_name", "xsum",
                "--max_source_length", "512",
                "--run_name", "pegasus-xsum",
                // "--source_prefix", "summarize: ",
                "--do_eval",
                "--predict_with_generate",
                "--num_beams", "8",
                "--per_device_eval_batch_size", "8",
                "--overwrite_output_dir",
                "--output_dir", "/tmp/vscode-runs/pegasus"
            ]
        },
        {
            "name": "model analysis",
            "type": "python",
            "request": "launch",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
            },
            "justMyCode": false,
            "cwd": "${workspaceFolder}/nb",
            "program": "model_analysis.py",
            "args": [
            ]
        },
        {
            "name": "pt transformer tutorial",
            "type": "python",
            "request": "launch",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
            },
            "cwd": "${workspaceFolder}/nb",
            "program": "transformer_tutorial.py",
            "args": [
            ]
        },
        {
            "name": "[non-ddp] nncf bert squad - no compression",
            "type": "python",
            "request": "launch",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "dbg_mvmt_squad",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/question-answering/",
            "program": "run_qa.py",
            "args": [
                "--model_name_or_path", "bert-base-uncased",
                "--dataset_name", "squad",
                "--do_train",
                "--do_eval",
                "--per_device_train_batch_size", "16",
                "--learning_rate", "3e-5",
                "--num_train_epochs", "5",
                "--max_seq_length", "384",
                "--doc_stride", "128",
                "--lr_scheduler_type", "cosine_with_restarts",
                "--warmup_ratio", "0.05",
                // "--evaluation_strategy", "steps",
                // "--save_steps", "5", // only save once
                "--output_dir", "/tmp/vscode-runs/bert-squad/",
                "--run_name", "dbg_bert-base-uncased-mvmt",
                "--nncf_config",
                "mvmt_config/nncf_bert_squad.json",
                // "mvmt_config/nncf_bert_squad_mvmt_ssbs_qat_kd.json",
                // "mvmt_config/nncf_bert_squad_mvmt_32x32.json",
                // "mvmt_config/nncf_bert_squad_mvmt.json",
                // "nncf_bert_config_squad_mvnt_pruning-distill-cosinelr.json",
                // "mvmt_config/nncf_bert_squad_mvmt_kd.json",
                // "${workspaceFolder}/transformers/dbg_nncf_bert_config_squad_mvnt_pruning.json",
                "--logging_steps", "1",
                "--overwrite_output_dir"
            ]
        },
        {
            "name": "[NNCF] ImageNet Test Int8",
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "cwd": "${workspaceFolder}/nncf/examples/torch/classification/",
            "program": "main.py",
            "args": [
                "-m", "test",
                "--gpu-id", "3",
                "--batch-size", "64",
                "--log-dir", "/tmp/vscode-runs/test",
                // "--data", "/data/dataset/imagenet/ilsvrc2012/torchvision",
                "--data", "/data/dataset/imagenet/ilsvrc2012/imgnet-train5k-val50k",
                "--config", "configs/quantization/resnet50_imagenet_int8.json",
                // "--config", "configs/quantization/resnet50_imagenet.json",
                // "--config", "configs/quantization/mobilenet_v2_imagenet.json",
            ],
            "console": "integratedTerminal"
        },
        {
            "name": "[non-ddp] MVMT MNLI",
            "type": "python",
            "request": "launch",
            "env": {
                "CUDA_VISIBLE_DEVICES": "1",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                // "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "dbg_mvmt_mnli",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/text-classification/",
            "program": "run_glue.py",
            "args": [
                "--model_name_or_path", "bert-base-uncased",
                "--task_name", "mnli",
                "--do_train",
                "--do_eval",
                "--per_device_train_batch_size", "64",
                "--learning_rate", "3e-5",
                "--num_train_epochs", "2",
                "--max_seq_length", "128",
                // "--evaluation_strategy", "steps",
                // "--save_steps", "5", // only save once
                "--output_dir", "/tmp/vscode-runs/jpq/bert-mvmt-mnli/",
                "--run_name", "dbg_bert_mvmt",
                "--nncf_config",
                "mvmt_cfg/nncf_bert_xnli_mvmt.json",
                "--logging_steps", "1",
                "--overwrite_output_dir"
            ]
        },
        {
            "name": "[HMQ] ",
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "WANDB_DIR": "/tmp/hmq/wandb_dir"
            },
            "cwd": "${workspaceFolder}/ai-research/HMQ",
            "program": "main.py",
            "args": [
                "--network_name", "mobilenet_v1",
                "--lr_start", "0.000005", 
                "--gamma", "64", 
                "--lr_coefficient", "0.005", 
                "--batch_size", "256", 
                "--n_epochs", "30", 
                "--target_compression", "8.0", 
                // "--fp16"
                "--data_dir", 
                "/data/dataset/imagenet/ilsvrc2012/torchvision/",
            ],
            "console": "integratedTerminal"
        },
        {
            "name": "[non-ddp][Distillation] MVMT Pruning bert-base-uncased for Squad",
            "type": "python",
            "request": "launch",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/question-answering/",
            "program": "run_qa.py",
            "args": [
                "--model_name_or_path", "bert-base-uncased",
                "--teacher", "vuiseng9/bert-base-uncased-squad",
                "--teacher_ratio", "0.9",
                "--dataset_name", "squad",
                "--do_train",
                "--do_eval",
                "--evaluation_strategy", "steps",
                "--eval_steps", "10",
                "--per_device_train_batch_size", "16",
                "--learning_rate", "3e-5",
                "--num_train_epochs", "2",
                "--max_seq_length", "384",
                "--doc_stride", "128",
                "--save_steps", "1000000", // only save once
                "--output_dir", "/tmp/vscode-runs/jpq/distill-bert-base-uncased-mvmt/",
                "--nncf_config", "${workspaceFolder}/transformers/dbg_nncf_bert_config_squad_mvnt_pruning.json",
                "--logging_steps", "1",
                "--overwrite_output_dir"
            ]
        },
        {
            "name": "[non-ddp] MVMT Pruning bert-base-uncased for Squad",
            "type": "python",
            "request": "launch",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                // "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "dbg_mvmt_squad",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/question-answering/",
            "program": "run_qa.py",
            "args": [
                "--model_name_or_path", "bert-base-uncased",
                "--dataset_name", "squad",
                "--do_train",
                "--do_eval",
                "--per_device_train_batch_size", "16",
                "--learning_rate", "3e-5",
                "--num_train_epochs", "5",
                "--max_seq_length", "384",
                "--doc_stride", "128",
                "--lr_scheduler_type", "cosine_with_restarts",
                "--warmup_ratio", "0.05",
                // "--evaluation_strategy", "steps",
                // "--save_steps", "5", // only save once
                "--output_dir", "/tmp/vscode-runs/jpq/bert-base-uncased-mvmt/",
                "--run_name", "dbg_bert-base-uncased-mvmt",
                "--nncf_config",
                "mvmt_config/nncf_bert_squad_mvmt_ssbs.json",
                // "mvmt_config/nncf_bert_squad_mvmt_ssbs_qat_kd.json",
                // "mvmt_config/nncf_bert_squad_mvmt_32x32.json",
                // "mvmt_config/nncf_bert_squad_mvmt.json",
                // "nncf_bert_config_squad_mvnt_pruning-distill-cosinelr.json",
                // "mvmt_config/nncf_bert_squad_mvmt_kd.json",
                // "${workspaceFolder}/transformers/dbg_nncf_bert_config_squad_mvnt_pruning.json",
                "--logging_steps", "1",
                "--overwrite_output_dir"
            ]
        },
        {
            "name": "MVMT Pruning bert-base-uncased for Squad",
            "type": "python",
            "request": "launch",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0,1,2,3",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/question-answering/",
            "module": "torch.distributed.launch",
            "args": [
                "--nproc_per_node=4", "run_qa.py",
                "--model_name_or_path", "bert-base-uncased",
                "--dataset_name", "squad",
                "--do_train",
                "--do_eval",
                "--per_device_train_batch_size", "6",
                "--learning_rate", "3e-5",
                "--num_train_epochs", "2",
                "--max_seq_length", "384",
                "--doc_stride", "128",
                "--save_steps", "1000000", // only save once
                "--output_dir", "/tmp/vscode-runs/jpq/bert-base-uncased-mvmt/",
                "--nncf_config", "${workspaceFolder}/transformers/nncf_bert_config_squad_mvnt_pruning.json",
                "--logging_steps", "1",
                "--overwrite_output_dir"
            ]
        },
        {
            "name": "Eval squadv1", 
            // this model has been pretrained and released by official HF
            // https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad
            // f1 = 93.1584
            // exact_match = 86.91
            // eval_samples = 10784
            "type": "python",
            "request": "launch",
            "env": {
                "CUDA_VISIBLE_DEVICES": "1",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/question-answering/",
            "program": "run_qa.py",
            "args": [
                "--model_name_or_path",
                "vuiseng9/bert-base-uncased-squad", 
                // "csarron/bert-base-uncased-squad-v1",
                // "bert-large-uncased-whole-word-masking-finetuned-squad",
                "--dataset_name", "squad",
                "--do_eval",
                "--per_device_eval_batch_size", "384",
                "--max_seq_length", "384",
                "--doc_stride", "128",
                "--output_dir", "/tmp/vscode-runs/jpq/eval-squad1",
                "--overwrite_output_dir"
            ]
        },
        {
            "name": "QAFT bert-base-uncased for Squad",
            "type": "python",
            "request": "launch",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0,1,2,3",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/question-answering/",
            "module": "torch.distributed.launch",
            "args": [
                "--nproc_per_node=4", "run_qa.py",
                "--model_name_or_path", "bert-base-uncased",
                "--dataset_name", "squad",
                "--do_train",
                "--do_eval",
                "--per_device_train_batch_size", "6",
                "--learning_rate", "3e-5",
                "--num_train_epochs", "2",
                "--max_seq_length", "384",
                "--doc_stride", "128",
                "--save_steps", "1000000", // only save once
                "--output_dir", "/tmp/vscode-runs/jpd/bert-base-uncased-qaft-squad-2epoch/",
                "--nncf_config", "${workspaceFolder}/transformers/nncf_bert_config_squad.json",
                "--overwrite_output_dir"
            ]
        },
        {
            "name": "Sparsity bert-base-uncased for Squad",
            "type": "python",
            "request": "launch",
            "env": {
                "CUDA_VISIBLE_DEVICES": "0,1,2,3",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/question-answering/",
            "module": "torch.distributed.launch",
            "args": [
                "--nproc_per_node=4", "run_qa.py",
                "--model_name_or_path", "bert-base-uncased",
                "--dataset_name", "squad",
                "--do_train",
                "--do_eval",
                "--per_device_train_batch_size", "6",
                "--learning_rate", "3e-5",
                "--num_train_epochs", "2",
                "--max_seq_length", "384",
                "--doc_stride", "128",
                "--save_steps", "1000000", // only save once
                "--output_dir", "/tmp/vscode-runs/jpd/bert-base-uncased-sparsity/",
                "--nncf_config", "${workspaceFolder}/transformers/nncf_bert_config_squad_sparsity.json",
                "--overwrite_output_dir"
            ]
        },
        {
            "name": "[NNCF] Resnet34 RB",
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "cwd": "${workspaceFolder}/nncf/examples/torch/classification/",
            "program": "main.py",
            "args": [
                "-m", "train",
                // "--gpu-id", "0",
                "--log-dir", "/tmp/vscode-runs/jpq/sparsity",
                "--data", 
                "/data/dataset/cifar10",
                // "/data/dataset/imagenet/ilsvrc2012/imgnet-train1k-val1k-dev",
                // "/data/dataset/imagenet/ilsvrc2012/torchvision",
                "--config",
                "configs/sparsity/resnet34_cifar_rb_sparsity.json", 
                // "configs/quantization/resnet18_imagenet.json",
                // "/home/vchua/spd/nncf/examples/torch/classification/configs/pruning/resnet34_imagenet.json",
                // "--config", "configs/quantization/resnet50_imagenet.json",
                // "--config", "configs/quantization/mobilenet_v2_imagenet.json",
            ],
            "console": "integratedTerminal"
        },
    ]
}