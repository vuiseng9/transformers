{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "bert_onnx_mapper",
            // this model has been pretrained and released by official HF
            // https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad
            // f1 = 93.1584
            // exact_match = 86.91
            // eval_samples = 10784
            "type": "python",
            "request": "launch",
            "env": {
                "CUDA_VISIBLE_DEVICES": "2",
                // "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                // // "WANDB_MODE": "disabled",
                // "WANDB_PROJECT": "dbg_sparsity_squad",
                // "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/audio-classification/reporter",
            "justMyCode": false,
            "program": "bert_onnx_mapper.py",
            "args": []
        },
        {
            "name": "[dev] w2v2 keyword spotting",
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                // "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "dev-w2v2opt",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/audio-classification",
            "program": "run_audio_classification.py",
            "args": [
                "--model_name_or_path",
                // "/data/vchua/run/temp/wav2vec2-base-ft-keyword-spotting",
                "anton-l/wav2vec2-base-ft-keyword-spotting",
                "--dataset_name",
                "superb",
                "--dataset_config_name",
                "ks",
                "--output_dir",
                "/tmp/vscode-dev/dev-w2v2-keyword-qat",
                "--overwrite_output_dir",
                "--remove_unused_columns",
                "False",
                "--do_eval",
                "--do_train",
                "--nncf_config",
                "${workspaceFolder}/transformers/examples/pytorch/audio-classification/cfg-nncf/dev_nncf_wav2vec2_config.json",
                "--learning_rate",
                "3e-5",
                "--max_length_seconds",
                "1",
                "--attention_mask",
                "False",
                "--warmup_ratio",
                "0.1",
                "--num_train_epochs",
                "1",
                "--per_device_train_batch_size",
                "32",
                "--gradient_accumulation_steps",
                "4",
                "--per_device_eval_batch_size",
                "32",
                "--dataloader_num_workers",
                "4",
                "--logging_strategy",
                "steps",
                "--logging_steps",
                "1",
                "--evaluation_strategy",
                "epoch",
                "--save_strategy",
                "epoch",
                "--load_best_model_at_end",
                "True",
                "--metric_for_best_model",
                "accuracy",
                "--save_total_limit",
                "3",
                "--seed",
                "0",
                "--run_name",
                "w2v2-ac-qat",
                "--max_steps",
                "15",
                // "--teacher", "anton-l/wav2vec2-base-ft-keyword-spotting",
                // "--teacher_ratio", "0.9",
                // "--lr_scheduler_type", "cosine_with_restarts"
            ]
        },
        {
            "name": "[JPQD] w2v2 keyword spotting",
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "1",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                // "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "dev-w2v2opt",
                "WANDB_WATCH": "false",
                "YUJIE_SIGMOID_THRESHOLD": "0",
                "YUJIE_GLOBAL_LR": "1",
                "YUJIE_MASK_USE_GEQ": "0",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/audio-classification",
            "program": "run_audio_classification.py",
            "args": [
                "--model_name_or_path",
                // "/data/vchua/run/temp/wav2vec2-base-ft-keyword-spotting",
                "anton-l/wav2vec2-base-ft-keyword-spotting",
                "--dataset_name",
                "superb",
                "--dataset_config_name",
                "ks",
                "--output_dir",
                "/tmp/guest-vscode-dev/dev-w2v2-keyword-mvmt-crop",
                "--manual_crop",
                // "--skip_quantize",
                "--overwrite_output_dir",
                "--remove_unused_columns",
                "False",
                "--do_eval",
                // "--do_train",
                "--nncf_config",
                // "yujie-nncf_w2v2b_ks_mvmt-ct0-wr0.05-withqat-4ft-fixdisable.json",
                "yujie-nncf_w2v2b_ks_mvmt-const-t0.5-warm-r0.1-comp.json",
                // "/nvme2/yujiepan/workspace/jpqd-ac/LOGS/ww36/single-0824.0ab6-w2v2bks-ct0.5-wr0.1-withsig-bugv4/yujie-nncf_w2v2b_ks_mvmt-const-t0.5-warm-r0.1-comp.json",
                // "/home/yujiepan/work2/jpqd-ac/LOGS/ww36/0830.6ede-w2v2-jpqd-ct0-wr.05-nosig-globalLR-4ft-25epo-disable-overflowfix-warm1epo-lr2e-4/yujie-nncf_w2v2b_ks_mvmt-ct0-wr0.05-withqat-4ft-fixdisable.json", 
                // "${workspaceFolder}/transformers/examples/pytorch/audio-classification/cfg-nncf/dev_nncf_wav2vec2_config.json",
                "--gen_onnx",
                "--manual_load",
                // "/nvme1/vchua/run/p3-w2v2/w2v2b-ks/jpqd/run38.1-w2v2b-ks-mvmt-bt-25eph-warmup4to9-r0.05-alpha0.9-noloadbest/checkpoint-6600",
                "/home/yujiepan/work2/jpqd-ac/LOGS/ww36/single-0824.0ab6-w2v2bks-ct0.5-wr0.1-withsig-bugv4/checkpoint-4600",
                // "/home/yujiepan/work2/jpqd-ac/LOGS/ww36/0830.6ede-w2v2-jpqd-ct0-wr.05-nosig-globalLR-4ft-25epo-disable-overflowfix-warm1epo-lr2e-4/checkpoint-9800",
                "--learning_rate",
                "2e-4",
                "--max_length_seconds",
                "1",
                "--attention_mask",
                "False",
                "--warmup_ratio",
                "0.04",
                "--num_train_epochs",
                "25",
                "--per_device_train_batch_size",
                "32",
                "--gradient_accumulation_steps",
                "4",
                "--per_device_eval_batch_size",
                "32",
                "--dataloader_num_workers",
                "4",
                "--logging_strategy",
                "steps",
                "--logging_steps",
                "1",
                "--evaluation_strategy",
                "steps",
                "--eval_steps",
                "200",
                "--save_steps",
                "200",
                "--load_best_model_at_end",
                "True",
                "--metric_for_best_model",
                "accuracy",
                "--save_total_limit",
                "3",
                "--seed",
                "0",
                "--run_name",
                "w2v2-ac-jpqd",
                "--max_steps",
                "15",
                "--teacher",
                "anton-l/wav2vec2-base-ft-keyword-spotting",
                "--teacher_ratio",
                "0.9",
                // "--lr_scheduler_type", "cosine_with_restarts"
            ]
        },
        {
            "name": "[QAT] w2v2 keyword spotting",
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                // "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "dev-w2v2opt",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/audio-classification",
            "program": "run_audio_classification.py",
            "args": [
                "--model_name_or_path",
                "anton-l/wav2vec2-base-ft-keyword-spotting",
                "--dataset_name",
                "superb",
                "--dataset_config_name",
                "ks",
                "--output_dir",
                "/tmp/vscode-dev/w2v2-keyword-qat",
                "--overwrite_output_dir",
                "--remove_unused_columns",
                "False",
                "--do_eval",
                "--do_train",
                "--nncf_config",
                "${workspaceFolder}/optimum-openvino/optimum/intel/nncf/configs/nncf_wav2vec2_config.json",
                "--learning_rate",
                "3e-5",
                "--max_length_seconds",
                "1",
                "--attention_mask",
                "False",
                "--warmup_ratio",
                "0.1",
                "--num_train_epochs",
                "5",
                "--per_device_train_batch_size",
                "32",
                "--gradient_accumulation_steps",
                "4",
                "--per_device_eval_batch_size",
                "32",
                "--dataloader_num_workers",
                "4",
                "--logging_strategy",
                "steps",
                "--logging_steps",
                "1",
                "--evaluation_strategy",
                "epoch",
                "--save_strategy",
                "epoch",
                "--load_best_model_at_end",
                "True",
                "--metric_for_best_model",
                "accuracy",
                "--save_total_limit",
                "3",
                "--seed",
                "0",
                "--run_name",
                "w2v2-ac-qat",
                // "--max_steps", "5",
            ]
        },
        {
            "name": "[Eval] w2v2 keyword spotting",
            "type": "python",
            "request": "launch",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",
                "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
                "WANDB_MODE": "disabled",
                "WANDB_PROJECT": "summarization",
                "WANDB_WATCH": "false",
            },
            "cwd": "${workspaceFolder}/transformers/examples/pytorch/audio-classification",
            "program": "run_audio_classification.py",
            "args": [
                "--model_name_or_path",
                "anton-l/wav2vec2-base-ft-keyword-spotting",
                "--dataset_name",
                "superb",
                "--dataset_config_name",
                "ks",
                "--output_dir",
                "/tmp/vscode-dev/w2v2-keyword",
                "--overwrite_output_dir",
                "--remove_unused_columns",
                "False",
                "--do_eval",
                "--max_length_seconds",
                "1",
                "--attention_mask",
                "False",
                "--per_device_eval_batch_size",
                "32",
                "--dataloader_num_workers",
                "4",
                "--seed",
                "0"
            ]
        },
    ]
}